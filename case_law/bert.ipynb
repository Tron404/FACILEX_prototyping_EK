{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘model’: File exists\n",
      "/home/keddie/Desktop/job_nlp/FACILEX/case_law/model\n",
      "fatal: destination path 'paraphrase-multilingual-mpnet-base-v2' already exists and is not an empty directory.\n",
      "fatal: destination path 'distiluse-base-multilingual-cased-v2' already exists and is not an empty directory.\n",
      "/home/keddie/Desktop/job_nlp/FACILEX/case_law\n"
     ]
    }
   ],
   "source": [
    "!mkdir model\n",
    "%cd model\n",
    "!git clone https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
    "!git clone https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case c-543/deutsche telekom agvbundesrepublik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avis juridique important|61997jjudgment septem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avis juridique important|61982jjudgment (fourt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avis juridique important|61973jjudgment may 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avis juridique important|61995jjudgment (sixth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  case c-543/deutsche telekom agvbundesrepublik ...\n",
       "1  avis juridique important|61997jjudgment septem...\n",
       "2  avis juridique important|61982jjudgment (fourt...\n",
       "3  avis juridique important|61973jjudgment may 19...\n",
       "4  avis juridique important|61995jjudgment (sixth..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data_path = \"CJEU/inputdata/full_texts_all_cases/\"\n",
    "stop_words = pickle.load(open(\"stopwords.pickle\", \"rb\"))\n",
    "stop_words.extend(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocessing(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[\\n]+\", \"\", text)\n",
    "    text = re.sub(r\" {2,}\", \" \", text)\n",
    "\n",
    "    for stop_token in stop_words:\n",
    "        text = re.sub(\" \" + stop_token + r\"[,.; ]\", \" \", text)\n",
    "    text = re.sub(r\"[+]+\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "texts = []\n",
    "file_dict = {}\n",
    "for case in os.listdir(data_path):\n",
    "    #  if case.split(\"_\")[-1][:-4] in sampled_idx:\n",
    "    texts.append(open(data_path + case).read())\n",
    "    file_dict[case.split(\"_\")[-1][:-4]] = texts[-1]\n",
    "\n",
    "values = []\n",
    "key = {}\n",
    "counter = 0\n",
    "for k,v in file_dict.items():\n",
    "    values.append(v)\n",
    "    key[k] = counter\n",
    "    counter+=1\n",
    "\n",
    "df = pd.DataFrame({\"text\": texts})\n",
    "df[\"text\"] = df[\"text\"].apply(preprocessing)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def tfidf_encoding(data, test_data):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    def tfidf_processing(text: str):\n",
    "        final_text = \"\"\n",
    "        for sentence in sent_tokenize(text):\n",
    "            tokens = word_tokenize(sentence)\n",
    "            tokens = [stemmer.stem(token) for token in tokens]\n",
    "            final_text += \" \".join(tokens) + \" \"\n",
    "        return final_text\n",
    "\n",
    "    text = data[\"text\"].apply(tfidf_processing)\n",
    "\n",
    "    tfidf_model = TfidfVectorizer(use_idf = True, stop_words = stop_words)\n",
    "    tfidf_data = tfidf_model.fit_transform(text)\n",
    "\n",
    "    text = test_data[\"text\"].apply(tfidf_processing)\n",
    "    test_data = tfidf_model.transform(text)\n",
    "\n",
    "    return tfidf_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def encode_sentence(sentence: str) -> torch.tensor:\n",
    "    global idx\n",
    "    sys.stdout.write(\"\\r\" + f\"{idx}/{len(df)}\")\n",
    "    idx += 1\n",
    "    global model\n",
    "\n",
    "    return model.encode(sentence)\n",
    "\n",
    "def trans_encode(data, test_data):\n",
    "    return data[\"text\"].apply(encode_sentence).to_list(), test_data[\"text\"].apply(encode_sentence).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Importing sample cases...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "import scipy as sp\n",
    "\n",
    "def get_sample_cases(topic):\n",
    "    data = pd.read_csv(\"CJEU/inputdata/sampled_cases.csv\")\n",
    "    relevant_rows = data[data['source_case_topic'] == topic]\n",
    "    return relevant_rows['source'].tolist()\n",
    "\n",
    "def find_similar(tfidf_matrix, index, top_n):\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]\n",
    "\n",
    "\n",
    "print(\"* Importing sample cases...\")\n",
    "# Celex numbers of reference cases\n",
    "publichealth = get_sample_cases('public health')\n",
    "socialpolicy = get_sample_cases('social policy')\n",
    "dataprotection = get_sample_cases('data protection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>case c-543/deutsche telekom agvbundesrepublik ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>judgment (tenth chamber)october (*)‛protection...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\t\\t\\t\\tarrêt de la cour \\t\\t\\t c-135/commiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>judgment (third chamber)june (*)‛reference — p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>judgment (first chamber)october (*)(appeal — r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>avis juridique important|61993jjudgment februa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>order april – vischim v commission(case c-459/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>avis juridique important|61984jjudgment april ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>judgment (sixth chamber)march (*)‛social polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>judgment (second chamber)march ()*‛reference —...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    case c-543/deutsche telekom agvbundesrepublik ...\n",
       "1    judgment (tenth chamber)october (*)‛protection...\n",
       "2     \\t\\t\\t\\tarrêt de la cour \\t\\t\\t c-135/commiss...\n",
       "3    judgment (third chamber)june (*)‛reference — p...\n",
       "4    judgment (first chamber)october (*)(appeal — r...\n",
       "..                                                 ...\n",
       "172  avis juridique important|61993jjudgment februa...\n",
       "173  order april – vischim v commission(case c-459/...\n",
       "174  avis juridique important|61984jjudgment april ...\n",
       "175  judgment (sixth chamber)march (*)‛social polic...\n",
       "176  judgment (second chamber)march ()*‛reference —...\n",
       "\n",
       "[177 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = publichealth + socialpolicy + dataprotection\n",
    "\n",
    "texts = []\n",
    "file_dict = {}\n",
    "for case in os.listdir(data_path):\n",
    "    texts.append(open(data_path + case).read())\n",
    "    file_dict[case.split(\"_\")[-1][:-4]] = texts[-1]\n",
    "\n",
    "texts = [text for case_id, text  in file_dict.items() if case_id in test_data]\n",
    "test_df = pd.DataFrame({\"text\": texts})\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(preprocessing)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Import citations for cases...\n",
      " Successfully imported citations!\n"
     ]
    }
   ],
   "source": [
    "print(\"* Import citations for cases...\")\n",
    "citations = pd.read_csv('CJEU/inputdata/all_cases_citations.csv')\n",
    "print(\" Successfully imported citations!\")\n",
    "\n",
    "def find_cited_cases(celexnumber):\n",
    "    global citations\n",
    "    relevantsource = citations[citations['source'] == celexnumber]\n",
    "    return relevantsource['target'].tolist()\n",
    "\n",
    "def exists_citation_link_between(celexnumber1,celexnumber2):\n",
    "    global citations\n",
    "    relevantsource1 = citations[citations['source'] == celexnumber1]\n",
    "    relevantsource2 = citations[citations['source'] == celexnumber2]\n",
    "    if celexnumber2 in relevantsource1['target'].tolist() or celexnumber1 in relevantsource2['target'].tolist():\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_encoding\n",
      "trans_encode\n",
      "14004/13828trans_encode\n",
      "14004/13828"
     ]
    }
   ],
   "source": [
    "dict_res = {}\n",
    "models = [\"distiluse-base-multilingual-cased-v2\", \"paraphrase-multilingual-mpnet-base-v2\"]\n",
    "encoded_dict = {}\n",
    "\n",
    "# Keep a record of document to index\n",
    "def get_doc_index(docid):\n",
    "    global key\n",
    "    rowid = key[docid]\n",
    "    return rowid\n",
    "    \n",
    "# Keep a record of document to index\n",
    "def get_doc_row(docid, data):\n",
    "    global key\n",
    "    rowid = key[docid]\n",
    "    row = data[rowid,:]\n",
    "    return row\n",
    "\n",
    "# Keep a record of document to index\n",
    "def get_doc_id(rowid):\n",
    "    global key\n",
    "    for k, v in key.items():    \n",
    "        if v == rowid:\n",
    "            return k\n",
    "    return -1\n",
    "\n",
    "# Function to convert entire similarity results to case ID references\n",
    "def convert_to_case_references(tfidf_result):\n",
    "    result = []\n",
    "    for item in tfidf_result:\n",
    "        case_reference = get_doc_id(item[0]) # convert to case reference\n",
    "        similarity_value = item[1]\n",
    "        result.append((case_reference,similarity_value))\n",
    "    return result\n",
    "\n",
    "def lookup_similar_cases(sample_cases, n, topic, data):\n",
    "    global results\n",
    "    for item in sample_cases:\n",
    "        index = get_doc_index(item)                         # Look up this cases index in the TFIDF matrix\n",
    "        similar_cases = find_similar(data, index, n)  # Look up top n similar cases for this case\n",
    "        similar_cases_references = convert_to_case_references(similar_cases)\n",
    "        for reference in similar_cases_references:\n",
    "            results.append([item,reference[0],reference[1],'tfidf',exists_citation_link_between(item,reference[0]),topic])\n",
    "\n",
    "for encoding_method in [\"tfidf\", \"distiluse\", \"mpnet\"]:\n",
    "# for encoding_method in [\"tfidf\"]:\n",
    "    results = []\n",
    "    idx = 0\n",
    "\n",
    "    data = None\n",
    "    if encoding_method == \"tfidf\":\n",
    "        encoding_func = tfidf_encoding\n",
    "    elif encoding_method == \"distiluse\":\n",
    "        model_path = f\"model/{models[0]}\"\n",
    "        model = SentenceTransformer(model_path).to(device)\n",
    "        encoding_func = trans_encode\n",
    "\n",
    "    else:\n",
    "        model_path = f\"model/{models[1]}\"\n",
    "        model = SentenceTransformer(model_path).to(device)\n",
    "        encoding_func = trans_encode\n",
    "\n",
    "    print(encoding_func.__name__)\n",
    "    data, test_data = encoding_func(df, test_df)\n",
    "    encoded_dict[encoding_method] = (data, test_data)\n",
    "\n",
    "    # print(\"* Computing similar cases...\")\n",
    "    # # 1. Public Health\n",
    "    # lookup_similar_cases(publichealth,20,'public health', data)\n",
    "    # # # 2. Social Policy\n",
    "    # lookup_similar_cases(socialpolicy,20,'social policy', data)\n",
    "    # # # 3. Data Protection\n",
    "    # lookup_similar_cases(dataprotection,20,'data protection', data)\n",
    "\n",
    "    # dict_res[encoding_method] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Computing similar cases...\n",
      "* Computing similar cases...\n",
      "* Computing similar cases...\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "models = [\"distiluse-base-multilingual-cased-v2\", \"paraphrase-multilingual-mpnet-base-v2\"]\n",
    "\n",
    "# Keep a record of document to index\n",
    "def get_doc_index(docid):\n",
    "    global key\n",
    "    rowid = key[docid]\n",
    "    return rowid\n",
    "    \n",
    "# Keep a record of document to index\n",
    "def get_doc_row(docid, data):\n",
    "    global key\n",
    "    rowid = key[docid]\n",
    "    row = data[rowid,:]\n",
    "    return row\n",
    "\n",
    "# Keep a record of document to index\n",
    "def get_doc_id(rowid):\n",
    "    global key\n",
    "    for k, v in key.items():    \n",
    "        if v == rowid:\n",
    "            return k\n",
    "    return -1\n",
    "\n",
    "# Function to convert entire similarity results to case ID references\n",
    "def convert_to_case_references(tfidf_result):\n",
    "    result = []\n",
    "    for item in tfidf_result:\n",
    "        case_reference = get_doc_id(item[0]) # convert to case reference\n",
    "        similarity_value = item[1]\n",
    "        result.append((case_reference,similarity_value))\n",
    "    return result\n",
    "\n",
    "def lookup_similar_cases(sample_cases, n, topic, data, method):\n",
    "    global results\n",
    "    for item in sample_cases:\n",
    "        index = get_doc_index(item)                         # Look up this cases index in the TFIDF matrix\n",
    "        similar_cases = find_similar(data, index, n)  # Look up top n similar cases for this case\n",
    "        similar_cases_references = convert_to_case_references(similar_cases)\n",
    "        for reference in similar_cases_references:\n",
    "            results.append([item,reference[0],reference[1],method,exists_citation_link_between(item,reference[0]),topic])\n",
    "\n",
    "for encoding_method in [\"tfidf\", \"distiluse\", \"mpnet\"]:\n",
    "# for encoding_method in [\"tfidf\"]:\n",
    "    results = []\n",
    "    idx = 0\n",
    "\n",
    "    (data, test_data) = encoded_dict[encoding_method]\n",
    "\n",
    "    print(\"* Computing similar cases...\")\n",
    "    # 1. Public Health\n",
    "    lookup_similar_cases(publichealth,20,'public health', data, encoding_method)\n",
    "    # # 2. Social Policy\n",
    "    lookup_similar_cases(socialpolicy,20,'social policy', data, encoding_method)\n",
    "    # # 3. Data Protection\n",
    "    lookup_similar_cases(dataprotection,20,'data protection', data, encoding_method)\n",
    "\n",
    "    all_results += results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_case</th>\n",
       "      <th>similar_case</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>method</th>\n",
       "      <th>citation_link</th>\n",
       "      <th>source_case_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62003CJ0453</td>\n",
       "      <td>62006CO0421</td>\n",
       "      <td>0.7273723559334702</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>1</td>\n",
       "      <td>public health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62003CJ0453</td>\n",
       "      <td>61984CJ0028</td>\n",
       "      <td>0.6648683721282975</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0</td>\n",
       "      <td>public health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62003CJ0453</td>\n",
       "      <td>61984CJ0195</td>\n",
       "      <td>0.6189330729443988</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0</td>\n",
       "      <td>public health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62003CJ0453</td>\n",
       "      <td>61990CJ0039</td>\n",
       "      <td>0.5797361556748202</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0</td>\n",
       "      <td>public health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62003CJ0453</td>\n",
       "      <td>62002CJ0145</td>\n",
       "      <td>0.5667513340809365</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0</td>\n",
       "      <td>public health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_case similar_case    similarity_score method citation_link  \\\n",
       "0  62003CJ0453  62006CO0421  0.7273723559334702  tfidf             1   \n",
       "1  62003CJ0453  61984CJ0028  0.6648683721282975  tfidf             0   \n",
       "2  62003CJ0453  61984CJ0195  0.6189330729443988  tfidf             0   \n",
       "3  62003CJ0453  61990CJ0039  0.5797361556748202  tfidf             0   \n",
       "4  62003CJ0453  62002CJ0145  0.5667513340809365  tfidf             0   \n",
       "\n",
       "  source_case_topic  \n",
       "0     public health  \n",
       "1     public health  \n",
       "2     public health  \n",
       "3     public health  \n",
       "4     public health  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_res = {column: np.asarray(all_results)[:, idx] for column, idx in zip([\"source_case\",\"similar_case\",\"similarity_score\",\"method\",\"citation_link\",\"source_case_topic\"], range(0,6))}\n",
    "df = pd.DataFrame(dict_res)\n",
    "df.to_csv(\"results.csv\", index = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005649717514123272\n"
     ]
    }
   ],
   "source": [
    "def rank(val,a):\n",
    "    if sp.sparse.issparse(a):\n",
    "        return a[a>=val].shape[1] #if a is sparse\n",
    "    return len(a[a>=val]) # if a is dense\n",
    "\n",
    "def reciprocal_rank(l1_vecs, l2_vecs):\n",
    "    '''Mean reciprocal rank'''\n",
    "    if torch.is_tensor(l1_vecs):\n",
    "        l1_vecs, l2_vecs = l1_vecs.cpu().detach().numpy(), l2_vecs.cpu().detach().numpy()\n",
    "        \n",
    "    sim = cosine_similarity(l1_vecs, l2_vecs)\n",
    "\n",
    "    return sum([1/rank(sim[i,:sim.shape[1]],sim[i]) for i in range(sim.shape[0])])/sim.shape[0]\n",
    "\n",
    "\n",
    "for encoding_method, (data, test_data) in encoded_dict.items():\n",
    "    print(reciprocal_rank(data, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict_res, open(\"res.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/keddie/Desktop/job_nlp/FACILEX/case_law/bert.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/keddie/Desktop/job_nlp/FACILEX/case_law/bert.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39;49masarray(results)[:, \u001b[39m4\u001b[39;49m]\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "np.sum(np.asarray(results)[:, 4].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MEAN-POOLING WITH ATTENTION FOR NON-SENTENCE TRANSFORMERS\n",
    "# from transformers import BatchEncoding\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, fast_tokenizer = True)\n",
    "# model = AutoModel.from_pretrained(model_path).to(device)\n",
    "\n",
    "# def encode_sentence_transformer(sentence: list) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Given a list of tokens, compute its dense vector using a BERT model. The resulting 512 tokens are mean-pooled, \n",
    "#     taking into consideration their presence in the attention mask.\n",
    "#     \"\"\"\n",
    "#     sentence = \" \".join(sentence)\n",
    "#     tokenized_sentence = tokenizer(sentence, max_length = 512, padding = \"max_length\", truncation = True, return_tensors = \"pt\", return_attention_mask = True)\n",
    "    \n",
    "#     # cast all tensors to the same device -> speeds up prediction time and any subsequent computations\n",
    "#     aux = {}\n",
    "#     for key, value in tokenized_sentence.items():\n",
    "#         aux[key] = value.to(device)\n",
    "#     tokenized_sentence = BatchEncoding(aux)\n",
    "    \n",
    "#     model_output = model(**tokenized_sentence)\n",
    "\n",
    "#     # pool the values of the last hidden state using a weighted averaged that ignores all tokens with 0 in the attention mask\n",
    "#     last_hidden_state = model_output.last_hidden_state[0]\n",
    "#     attention_mask = tokenized_sentence.attention_mask[0]\n",
    "#     attention_mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "#     data_attention = last_hidden_state * attention_mask\n",
    "#     sentence_vector = torch.sum(data_attention, 0)/torch.clamp(attention_mask.sum(0), min = 1e-9)\n",
    "\n",
    "#     return np.array(sentence_vector.cpu().detach().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facilex_caselaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
