{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "multi_lexsum = load_dataset(\"allenai/multi_lexsum\", name=\"v20230518\")\n",
    "modified_dataset = multi_lexsum[\"test\"].filter(lambda x: x[\"summary/short\"] != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "# import torch\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5\")\n",
    "\n",
    "# embedding_model = SentenceTransformer(\"../models/multi-qa-mpnet-base-dot-v1\", device = device).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The plaintiffs filed a lawsuit on March 8, 2014, alleging that the City of Montgomery, Alabama, improperly imprisoned them for failing to pay traffic fines. They alleged that they did not have an ability to pay the fines due to their financial circumstances and that the city did not consider their ability to pay. On May 1, 2014, the District Court granted the plaintiffs motion for a preliminary injunction, preventing the city from collecting more money from traffic tickets of plaintiffs'. On October 31, 2014 the parties filed to dismiss the case pursuant to a settlement agreement, which included numerous changes to Municipal Court proceedings. The case is now closed.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_dataset[\"summary/short\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "# user_prompt = \"Summarize concisely the following legal texts. Include as many relevant facts as possible. A fact is relevant if it mentions plaintiffs, counsel, type of action, filling date, name of the court, description of class, defendants, statuatory basis, rought remedy, judges, consolidated class, whether it is a class action, date of decree, citations, duration of decrees, last action in case.\"\n",
    "system_prompt = \"You are a legal expert. You must answer concisely and truthfully, including only information that is relevant to the conversation. Stay faithful to the original text and keep the exact wording as found in the text as closely as possible. Only include facts relevant to the text, without any filler words.\"\n",
    "\n",
    "max_context_length = 16384\n",
    "max_output_length = 130\n",
    "length_system_prompt = len(tokenizer.encode(system_prompt))\n",
    "print(length_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from env_utils import load_env_from_file\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_env_from_file(\".\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_prompt_cod = \"\"\"TEXT:<INPUT>{INPUT}</INPUT>\n",
    "# Generate increasingly concise, fact-dense summaries of the text above. Repeat the following 2 steps 3 times:\n",
    "# Step 1. Identify 1-3 relevant facts from the INPUT which are missing from the previously generated summary.\n",
    "# Step 2. Write a new, denser summary of identical length which covers all facts from the previous summary that includes the relevant facts you found.\n",
    "# A relevant fact is:\n",
    "# * plaintiffs\n",
    "# * counsel\n",
    "# * taken actions\n",
    "# * dates\n",
    "# * name of court\n",
    "# * defendants\n",
    "# * statutory basis\n",
    "# * sought remedy\n",
    "# * judges\n",
    "# * case consolidation\n",
    "# * class action\n",
    "# * date of decrees\n",
    "# * duration of decrees\n",
    "# * citations\n",
    "# * last action in the case\n",
    "# Guidelines:\n",
    "# * the first summary must be 130 words long, containing general information about the text\n",
    "# * rewrite the first summary and add more relevant facts\n",
    "# * make space by removing any filler words or phrases that are uninformative\n",
    "# * the summaries should be highly dense and concise\n",
    "# * relevant facts can be added anywhere in the summary\n",
    "# * never remove relevant facts from a previous summary. If it is not possible to make more space, add fewever new facts\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [02:10<00:00,  1.74s/it]\n",
      "100%|██████████| 75/75 [01:54<00:00,  1.52s/it]\n",
      "100%|██████████| 75/75 [01:43<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def check_prompt_length(prompt):\n",
    "    return len(tokenizer.encode(prompt)) + max_output_length + length_system_prompt > max_context_length\n",
    "\n",
    "def from_extracted(path, test_size, current_length, sentences=2):\n",
    "    summs = []\n",
    "    files = os.listdir(path)\n",
    "    files = sorted(files, key = lambda x: int(x.split(\".\")[0]))\n",
    "    for file in files:\n",
    "        if int(file.split(\".\")[0]) < test_size:\n",
    "            docs = json.load(open(path + file, \"r\"))\n",
    "\n",
    "            doc = \"\".join([\"\".join(doc_sentences[:sentences]) for doc_sentences in docs])\n",
    "            if len(tokenizer.encode(doc)) + current_length > max_context_length:\n",
    "                doc = \"\"\n",
    "                sent_limit = sentences-1\n",
    "                while (doc == \"\" and len(doc) + current_length < max_context_length) and sent_limit > 0:\n",
    "                    doc = \"\".join([\"\".join(doc_sentences[:sent_limit]) for doc_sentences in docs])\n",
    "                    sent_limit -= 1\n",
    "\n",
    "            summs.append(doc)\n",
    "\n",
    "    return summs\n",
    "\n",
    "def completion_with_retry(**kwargs):\n",
    "    return client.chat.completions.create(**kwargs)\n",
    "\n",
    "user_prompt_summary_basic = \"Summarize the text:<INPUT>{INPUT}</INPUT>\\nSummary:\"\n",
    "# user_prompt_summary_detailed = \"Imagine you're a legal scholar tasked with distilling the essence of a complex case law into a concise and compelling summary. Your summary must encompass a rich tapestry of legal elements, including the identities of the plaintiffs and defendants, the brilliant minds behind the legal arguments (counsel), the decisive actions taken, the chronological tapestry of dates, the distinguished court presiding over the matter, the statutory framework underpinning the case, the sought-after remedy, the esteemed judges imparting wisdom, any case consolidations adding layers of complexity, the potential for class action ramifications, the pivotal date of decrees shaping the outcome, the temporal span of decree effectiveness, the authoritative citations grounding the legal analysis, and the climactic last action in the case. Your summary should weave these elements together seamlessly, ensuring a vivid and comprehensive portrayal of the legal landscape: <INPUT>{INPUT}</INPUT>\\nSummary:\"\n",
    "user_prompt_length = len(tokenizer.encode(user_prompt_summary_basic))\n",
    "test_size = 75\n",
    "extract_types = np.asarray([\"random_selection\", \"first5last5\", \"random_selection_bert\", \"first5last5_bert\"])\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "prompt_type = \"basic\"\n",
    "\n",
    "if model not in os.listdir(\"answers\"):\n",
    "    os.mkdir(f\"answers/{model}\")\n",
    "if prompt_type not in os.listdir(f\"answers/{model}\"):\n",
    "    os.mkdir(f\"answers/{model}/{prompt_type}\")\n",
    "\n",
    "for extract_sum_type in extract_types[1:]:\n",
    "    responses = []\n",
    "    basic_responses = []\n",
    "    path = f\"extracted_sums/extracted_sums_json_{extract_sum_type}/\"\n",
    "\n",
    "    extracted_summaries = from_extracted(path, test_size=test_size, sentences=2, current_length=user_prompt_length+length_system_prompt+max_output_length)\n",
    "\n",
    "    if extract_sum_type not in os.listdir(f\"answers/{model}/{prompt_type}/\"):\n",
    "        os.mkdir(f\"answers/{model}/{prompt_type}/{extract_sum_type}/\")\n",
    "\n",
    "    for idx, summ in enumerate(tqdm(extracted_summaries[:test_size])):\n",
    "        if f\"{idx}.json\" in os.listdir(f\"answers/{model}/{prompt_type}\"):\n",
    "            continue\n",
    "        basic_prompt = user_prompt_summary_basic.format(INPUT=summ)\n",
    "        message_history = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": basic_prompt\n",
    "                }\n",
    "            ]\n",
    "        chat_completion = completion_with_retry(\n",
    "            messages=message_history,\n",
    "            model=model,\n",
    "            temperature=0,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "        )\n",
    "\n",
    "        json.dump([*message_history, {\"role\": \"assistant\", \"content\": chat_completion.choices[0].message.content}], open(f\"answers/{model}/{prompt_type}/{extract_sum_type}/{idx}.json\", \"w\"), indent=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import pickle\n",
    "\n",
    "# def from_extracted(path, test_size):\n",
    "#     summs = []\n",
    "#     files = os.listdir(path)\n",
    "#     files = sorted(files, key = lambda x: int(x.split(\".\")[0]))\n",
    "#     for file in files:\n",
    "#         if int(file.split(\".\")[0]) < test_size:\n",
    "#             docs = json.load(open(path + file, \"r\"))\n",
    "#             doc = \"\".join([\"\".join(doc_sentences[:2]) for doc_sentences in docs])\n",
    "#             summs.append(doc)\n",
    "\n",
    "#     return summs\n",
    "\n",
    "# ## CoT summarization\n",
    "# test_size = 616\n",
    "# extract_types = np.asarray([\"random_selection\", \"first5last5\", \"random_selection_bert\", \"first5last5_bert\"])\n",
    "# prompt_type = \"1shot_cot_summarization\"\n",
    "# for extract_sum_type in extract_types[[0,2]]:\n",
    "#     responses = []\n",
    "#     basic_responses = []\n",
    "#     path = f\"extracted_sums/extracted_sums_json_{extract_sum_type}/\"\n",
    "\n",
    "#     user_prompt_summary_basic = \"Summarize the text below in 130 words. Let's think about it carefully, considering the importance of each fact in the final summary.\"\\\n",
    "#           + \"\\n\\nSOURCE:{{\\n{SOURCE}\\n}}\\n\\nSUMMARY:{{\\n{SUMMARY}\\n}}\\n\\nSOURCE:{{\\n{SOURCE_Q}\\n}}\\n\\nSUMMARY:\"\n",
    "#     user_prompt_revision = \"Let's have another look through the summary and source text. Include more important facts, namely: plaintiffs, counsel, taken actions, dates, name of court, defendants, statutory basis, sought remedy, judges, case consolidation, class action, date of decrees, duration of decrees, citations, last action in the case, but within 130 words.\"\n",
    "#     extracted_summaries = from_extracted(path, test_size=test_size)\n",
    "\n",
    "#     for summ in tqdm(extracted_summaries[1:]):\n",
    "#         basic_prompt = user_prompt_summary_basic.format(SOURCE=extracted_summaries[0], SUMMARY=modified_dataset[\"summary/short\"][0], SOURCE_Q=summ)\n",
    "#         # original summary\n",
    "#         completion = client.chat.completions.create(\n",
    "#             model = \"gpt-3.5-turbo-16k\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": system_prompt},\n",
    "#                 {\"role\": \"user\", \"content\": basic_prompt}\n",
    "#             ],\n",
    "#             frequency_penalty=0,\n",
    "#             presence_penalty=0,\n",
    "#             top_p=0.2,\n",
    "#             max_tokens=250,\n",
    "#             stop=[\"SOURCE\"]\n",
    "#         )\n",
    "\n",
    "#         basic_summary = completion.choices[0].message.content\n",
    "\n",
    "#         # # get elements from text\n",
    "#         completion = client.chat.completions.create(\n",
    "#             model = \"gpt-3.5-turbo-1161\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": system_prompt},\n",
    "#                 {\"role\": \"user\", \"content\": basic_prompt},\n",
    "#                 {\"role\": \"assistant\", \"content\": basic_summary},\n",
    "#                 {\"role\": \"user\", \"content\": user_prompt_revision}\n",
    "#             ],\n",
    "#             frequency_penalty=0,\n",
    "#             presence_penalty=0,\n",
    "#             top_p=0.2,\n",
    "#             max_tokens=250,\n",
    "#             stop=[\"SOURCE\"]\n",
    "#         )\n",
    "\n",
    "#         responses.append(completion.choices[0].message.content)\n",
    "#         basic_responses.append(basic_summary)\n",
    "\n",
    "#     pickle.dump(responses, open(f\"{test_size}_predicted_text_{prompt_type}_{extract_sum_type}.pickle\", \"wb\"))\n",
    "#     pickle.dump(basic_responses, open(f\"{test_size}_predicted_basic_text_{prompt_type}_{extract_sum_type}.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# ['led: rouge1: 45.89', 'led: rouge2: 23.00', 'led: rougeL: 31.17', 'led: rougeLsum: 32.01']\n",
    "# ['primera: rouge1: 42.87', 'primera: rouge2: 20.79', 'primera: rougeL: 29.31', 'primera: rougeLsum: 29.79']\n",
    "\n",
    "rouge_scoring = evaluate.load(\"rouge\")\n",
    "print(rouge_scoring.compute(predictions=responses, references=modified_dataset[:test_size][\"summary/short\"], use_stemmer = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple-random: {'rouge1': 0.3669443053395881, 'rouge2': 0.12299475909651572, 'rougeL': 0.2148706258231231, 'rougeLsum': 0.22091825483890215}\n",
    "# simple-5/5: {'rouge1': 0.3600777579472476, 'rouge2': 0.11723337466920167, 'rougeL': 0.21617088924165473, 'rougeLsum': 0.22142449133012626}\n",
    "# simple-randombert: {'rouge1': 0.3542897797369077, 'rouge2': 0.10311088709231235, 'rougeL': 0.2069714451531819, 'rougeLsum': 0.20992633649991677}\n",
    "# simple-5/5bert: {'rouge1': 0.35138148214931236, 'rouge2': 0.104968022054537, 'rougeL': 0.20538624503075653, 'rougeLsum': 0.20893048554674404}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facilex_caselaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
