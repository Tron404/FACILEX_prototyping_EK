{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "multi_lexsum = load_dataset(\"allenai/multi_lexsum\", name=\"v20230518\")\n",
    "modified_dataset = multi_lexsum[\"test\"].filter(lambda x: x[\"summary/short\"] != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5\")\n",
    "\n",
    "# embedding_model = SentenceTransformer(\"../models/multi-qa-mpnet-base-dot-v1\", device = device).half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The plaintiffs filed a lawsuit on March 8, 2014, alleging that the City of Montgomery, Alabama, improperly imprisoned them for failing to pay traffic fines. They alleged that they did not have an ability to pay the fines due to their financial circumstances and that the city did not consider their ability to pay. On May 1, 2014, the District Court granted the plaintiffs motion for a preliminary injunction, preventing the city from collecting more money from traffic tickets of plaintiffs'. On October 31, 2014 the parties filed to dismiss the case pursuant to a settlement agreement, which included numerous changes to Municipal Court proceedings. The case is now closed.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_dataset[\"summary/short\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# user_prompt = \"Summarize concisely the following legal texts. Include as many relevant facts as possible. A fact is relevant if it mentions plaintiffs, counsel, type of action, filling date, name of the court, description of class, defendants, statuatory basis, rought remedy, judges, consolidated class, whether it is a class action, date of decree, citations, duration of decrees, last action in case.\"\n",
    "system_prompt = \"You are a legal expert. You must answer concisely and truthfully, including only information that is relevant to the conversation. Stay faithful to the original text and keep the exact wording as found in the text as closely as possible. Only include facts relevant to the text, without any filler words.\"\n",
    "user_prompt = \"Summarize concisely the following legal texts. Include as many relevant facts as possible. Take a deep breath and think carefully which information is important for a summary.\"\n",
    "\n",
    "max_context_length = 16384\n",
    "max_output_length = 130\n",
    "length_user_prompt = len(tokenizer.encode(user_prompt))\n",
    "length_system_prompt = len(tokenizer.encode(system_prompt))\n",
    "print(length_user_prompt)\n",
    "print(length_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from env_utils import load_env_from_file\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "load_env_from_file(\".\")\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import pickle\n",
    "\n",
    "# def create_prompt(sources):\n",
    "#     doc = [\" \".join(doc_sentences) for doc_sentences in sources]\n",
    "#     prompt = user_prompt + \"\\n\" + \"{\" + \"\\n\".join(doc) + \"}\"\n",
    "\n",
    "#     return prompt\n",
    "\n",
    "# def check_prompt_length(prompt):\n",
    "#     return len(tokenizer.encode(prompt) + max_output_length + length_system_prompt)\n",
    "\n",
    "# def prompts_from_extracted(path, test_size):\n",
    "#     prompts = []\n",
    "#     files = os.listdir(path)\n",
    "#     files = sorted(files, key = lambda x: int(x.split(\".\")[0]))\n",
    "#     for file in files:\n",
    "#         if int(file.split(\".\")[0]) < test_size:\n",
    "#             prompts.append(create_prompt(json.load(open(path + file, \"r\"))))\n",
    "\n",
    "#     return prompts\n",
    "\n",
    "# test_size = 50\n",
    "# extract_types = [\"random_selection\", \"first5last5\", \"random_selection_bert\", \"first5last5_bert\"]\n",
    "# prompt_type = \"simple\"\n",
    "# for extract_sum_type in extract_types:\n",
    "#     path = f\"extracted_sums/extracted_sums_json_{extract_sum_type}/\"\n",
    "\n",
    "#     prompts = prompts_from_extracted(path, test_size=test_size)\n",
    "\n",
    "#     user_messages = {\"messages\": \n",
    "#         [\n",
    "#             {\"role\": \"user\", \"content\": prompt} for prompt in prompts\n",
    "#         ]\n",
    "#     }\n",
    "\n",
    "#     for idx, message in enumerate(user_messages[\"messages\"]):\n",
    "#         input_size = len(tokenizer.encode(message[\"content\"])) + max_output_length + length_system_prompt\n",
    "#         if input_size > max_context_length:\n",
    "#             print(\"Not good\", idx, input_size)\n",
    "#             exit()\n",
    "\n",
    "#     responses = []\n",
    "#     for message in tqdm(user_messages[\"messages\"]):\n",
    "#         completion = client.chat.completions.create(\n",
    "#             model = \"gpt-3.5-turbo-1106\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": system_prompt},\n",
    "#                 message\n",
    "#             ],\n",
    "#             frequency_penalty=0,\n",
    "#             temperature=0,\n",
    "#             presence_penalty=0\n",
    "#         )\n",
    "#         responses.append(completion.choices[0].message.content)\n",
    "\n",
    "#     pickle.dump(responses, open(f\"predicted_text_{prompt_type}_{extract_sum_type}.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The plaintiffs filed a lawsuit on March 8, 2014, alleging that the City of Montgomery, Alabama, improperly imprisoned them for failing to pay traffic fines. They alleged that they did not have an ability to pay the fines due to their financial circumstances and that the city did not consider their ability to pay. On May 1, 2014, the District Court granted the plaintiffs motion for a preliminary injunction, preventing the city from collecting more money from traffic tickets of plaintiffs'. On October 31, 2014 the parties filed to dismiss the case pursuant to a settlement agreement, which included numerous changes to Municipal Court proceedings. The case is now closed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def from_extracted(path, test_size):\n",
    "    summs = []\n",
    "    files = os.listdir(path)\n",
    "    files = sorted(files, key = lambda x: int(x.split(\".\")[0]))\n",
    "    for file in files:\n",
    "        if int(file.split(\".\")[0]) < test_size:\n",
    "            docs = json.load(open(path + file, \"r\"))\n",
    "            doc = \"\".join([\"\".join(doc_sentences) for doc_sentences in docs])\n",
    "            summs.append(doc)\n",
    "\n",
    "    return summs\n",
    "\n",
    "## CoT summarization\n",
    "test_size = 10\n",
    "extract_types = [\"random_selection\", \"first5last5\", \"random_selection_bert\", \"first5last5_bert\"]\n",
    "prompt_type = \"cot_summarization\"\n",
    "for extract_sum_type in extract_types[:1]:\n",
    "    responses = []\n",
    "    path = f\"extracted_sums/extracted_sums_json_{extract_sum_type}/\"\n",
    "\n",
    "    user_prompt_revision = \"Let's have another look through the summary and source text. Include more important facts, namely: plaintiffs, counsel, taken actions, dates, name of court, defendants, statutory basis, sought remedy, judges, case consolidation, class action, date of decrees, duration of decrees, citations, last action in the case, but within 130 words.\"\n",
    "    user_prompt_summary_basic = \"Summarize the text below in 130 words. Let's think about it carefully, considering the importance of each fact in the final summary.\\n\\nSOURCE: {{{SOURCE}}}\\n\\nSUMMARY:{{{SUMMARY}}}\\n\\nSOURCE:{{{SOURCE_Q}}}\\n\\nSUMMARY:\"\n",
    "    extracted_summaries = from_extracted(path, test_size=test_size)\n",
    "\n",
    "    for summ in tqdm(extracted_summaries[1:2]):\n",
    "        basic_prompt = user_prompt_summary_basic.format(SOURCE=extracted_summaries[0], SUMMARY=modified_dataset[\"summary/short\"][0], SOURCE_Q=summ)\n",
    "        # original summary\n",
    "        completion = client.chat.completions.create(\n",
    "            model = \"gpt-3.5-turbo-16k\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": basic_prompt}\n",
    "            ],\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            top_p=0.2,\n",
    "            temperature=0.2,\n",
    "            max_tokens=250\n",
    "        )\n",
    "\n",
    "        basic_summary = completion.choices[0].message.content\n",
    "        print(basic_summary)\n",
    "\n",
    "        # # get elements from text\n",
    "        prompt = user_prompt_revision\n",
    "        completion = client.chat.completions.create(\n",
    "            model = \"gpt-3.5-turbo-16k\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": basic_prompt},\n",
    "                {\"role\": \"assistant\", \"content\": basic_summary},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            top_p=0.2,\n",
    "            temperature=0.2,\n",
    "            max_tokens=250,\n",
    "        )\n",
    "\n",
    "        responses.append(completion.choices[0].message.content)\n",
    "\n",
    "    pickle.dump(responses, open(f\"predicted_text_{prompt_type}_{extract_sum_type}.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import pickle\n",
    "\n",
    "# def from_extracted(path, test_size):\n",
    "#     summs = []\n",
    "#     files = os.listdir(path)\n",
    "#     files = sorted(files, key = lambda x: int(x.split(\".\")[0]))\n",
    "#     for file in files:\n",
    "#         if int(file.split(\".\")[0]) < test_size:\n",
    "#             docs = json.load(open(path + file, \"r\"))\n",
    "#             doc = \"\".join([\"\".join(doc_sentences) for doc_sentences in docs])\n",
    "#             summs.append(doc)\n",
    "\n",
    "#     return summs\n",
    "\n",
    "# ## CoT summarization\n",
    "# test_size = 10\n",
    "# extract_types = [\"random_selection\", \"first5last5\", \"random_selection_bert\", \"first5last5_bert\"]\n",
    "# prompt_type = \"cot_summarization\"\n",
    "# for extract_sum_type in extract_types[:1]:\n",
    "#     responses = []\n",
    "#     path = f\"extracted_sums/extracted_sums_json_{extract_sum_type}/\"\n",
    "\n",
    "#     user_prompt_basic = \"Summarize concisely the following legal texts. Follow the wording of the text exactly:\\n{{{INPUT}}}\"\n",
    "#     user_prompt_extract_information = \"\"\"Consider the following text:\\n{{{INPUT}}}\n",
    "# Please extract the most relevant information from the text:\n",
    "# \"\"\"\n",
    "#     user_prompt_new_summary = \"\"\"SUMMARY:\\n{{{SUMMARY}}}\n",
    "# RELEVANT INFORMATION:\\n{{{INFO}}}\n",
    "# Refine the summary from SUMMARY such that it integrates information from RELEVANT INFORMATION, while using the same wording of the text:\"\"\"\n",
    "#     extracted_summaries = from_extracted(path, test_size=test_size)\n",
    "#     break\n",
    "\n",
    "#     for summ in tqdm(extracted_summaries[:3]):\n",
    "#         # original summary\n",
    "#         prompt = user_prompt_basic.format(INPUT = summ)\n",
    "#         completion = client.chat.completions.create(\n",
    "#             model = \"gpt-3.5-turbo-1106\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": system_prompt},\n",
    "#                 {\"role\": \"user\", \"content\": prompt}\n",
    "#             ],\n",
    "#             frequency_penalty=0,\n",
    "#             presence_penalty=0,\n",
    "#             top_p=0.1\n",
    "#         )\n",
    "#         basic_summary = completion.choices[0].message.content\n",
    "\n",
    "#         # # get elements from text\n",
    "#         prompt = user_prompt_extract_information.format(INPUT = summ)\n",
    "#         completion = client.chat.completions.create(\n",
    "#             model = \"gpt-3.5-turbo-1106\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": system_prompt},\n",
    "#                 {\"role\": \"user\", \"content\": prompt}\n",
    "#             ],\n",
    "#             frequency_penalty=0,\n",
    "#             presence_penalty=0,\n",
    "#             top_p=0.1\n",
    "#         )\n",
    "#         extracted_information = completion.choices[0].message.content\n",
    "\n",
    "#         # # get final summary\n",
    "#         # prompt = user_prompt_new_summary.format(SUMMARY = basic_summary, INFO = extracted_information)\n",
    "\n",
    "#         prompt = \"Please integrate the information you extracted into the summary, including as many events as possible. The resulting summary must have a maximum of 130 words. The output should follow this example: \\n{EXAMPLE}\".format(EXAMPLE = \"The plaintiffs filed a lawsuit on March 8, 2014, alleging that the City of Montgomery, Alabama, improperly imprisoned them for failing to pay traffic fines. They alleged that they did not have an ability to pay the fines due to their financial circumstances and that the city did not consider their ability to pay. On May 1, 2014, the District Court granted the plaintiffs motion for a preliminary injunction, preventing the city from collecting more money from traffic tickets of plaintiffs'. On October 31, 2014 the parties filed to dismiss the case pursuant to a settlement agreement, which included numerous changes to Municipal Court proceedings. The case is now closed.\")\n",
    "\n",
    "#         completion = client.chat.completions.create(\n",
    "#             model = \"gpt-3.5-turbo-1106\",\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": system_prompt},\n",
    "#                 {\"role\": \"assistant\", \"content\": basic_summary},\n",
    "#                 {\"role\": \"assistant\", \"content\": extracted_information},\n",
    "#                 {\"role\": \"user\", \"content\": prompt}\n",
    "#             ],\n",
    "#             frequency_penalty=0,\n",
    "#             presence_penalty=0,\n",
    "#             top_p=0.1\n",
    "#         )\n",
    "\n",
    "#         responses.append(completion.choices[0].message.content)\n",
    "\n",
    "#     pickle.dump(responses, open(f\"predicted_text_{prompt_type}_{extract_sum_type}.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.3303134287131386, 'rouge2': 0.10961782740750521, 'rougeL': 0.18189682569527046, 'rougeLsum': 0.1878133183123789}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# ['led: rouge1: 45.89', 'led: rouge2: 23.00', 'led: rougeL: 31.17', 'led: rougeLsum: 32.01']\n",
    "# ['primera: rouge1: 42.87', 'primera: rouge2: 20.79', 'primera: rougeL: 29.31', 'primera: rougeLsum: 29.79']\n",
    "\n",
    "rouge_scoring = evaluate.load(\"rouge\")\n",
    "print(rouge_scoring.compute(predictions=responses, references=modified_dataset[:test_size][\"summary/short\"], use_stemmer = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple-random: {'rouge1': 0.3669443053395881, 'rouge2': 0.12299475909651572, 'rougeL': 0.2148706258231231, 'rougeLsum': 0.22091825483890215}\n",
    "# simple-5/5: {'rouge1': 0.3600777579472476, 'rouge2': 0.11723337466920167, 'rougeL': 0.21617088924165473, 'rougeLsum': 0.22142449133012626}\n",
    "# simple-randombert: {'rouge1': 0.3542897797369077, 'rouge2': 0.10311088709231235, 'rougeL': 0.2069714451531819, 'rougeLsum': 0.20992633649991677}\n",
    "# simple-5/5bert: {'rouge1': 0.35138148214931236, 'rouge2': 0.104968022054537, 'rougeL': 0.20538624503075653, 'rougeLsum': 0.20893048554674404}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facilex_caselaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
