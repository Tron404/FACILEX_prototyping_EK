{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "multi_lexsum = load_dataset(\"allenai/multi_lexsum\", name=\"v20230518\")\n",
    "modified_dataset = multi_lexsum[\"test\"].filter(lambda x: x[\"summary/short\"] != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from summarizer import Summarizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from summarizer.sbert import SBertSummarizer\n",
    "# model_summ = SBertSummarizer(\"paraphrase-MiniLM-L6-v2\")\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_summ = Summarizer(\"distilbert-base-uncased\", hidden_concat = True, hidden = [-1, -2], gpu_id = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_extractive_summary(doc, limit_sentences = 10):\n",
    "    return model_summ(doc, use_first = False, return_as_list = True, num_sentences = limit_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select first 5 and last 5 docs\n",
    "def select_docs(docket, limit_docket_docs = 10):\n",
    "    half_docs = limit_docket_docs // 2\n",
    "\n",
    "    if len(docket) > 10:\n",
    "        subset_docs = docket[:half_docs] + docket[-half_docs:]\n",
    "    else:\n",
    "        subset_docs = docket\n",
    "\n",
    "    return subset_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 616/616 [3:04:09<00:00, 17.94s/it]  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "limit_docket = len(modified_dataset)\n",
    "limit_docket_docs = 10\n",
    "iterator = zip(modified_dataset[\"sources\"][:limit_docket], modified_dataset[\"sources_metadata\"][:limit_docket])\n",
    "path = f\"extracted_sums/extracted_sums_json_{'first5last5'}_bert\"\n",
    "if path.split(\"/\")[-1] not in os.listdir(\"extracted_sums/\"):\n",
    "    os.mkdir(path)\n",
    "for docket_id, (docket, docket_metadata) in enumerate(tqdm(iterator, total = limit_docket)):\n",
    "    if f\"{docket_id}.json\" in os.listdir(path):\n",
    "        continue\n",
    "\n",
    "    documents = select_docs(docket, limit_docket_docs = limit_docket_docs)\n",
    "\n",
    "    summaries = []\n",
    "    for doc in documents:\n",
    "        summary_aux = get_extractive_summary(doc, limit_sentences = 10)\n",
    "        summaries.append(summary_aux)\n",
    "    json.dump(summaries, open(f\"{path}/{docket_id}.json\", \"w\"), indent = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facilex_caselaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
