{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keddie/anaconda3/envs/groq_summary/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "multi_lexsum = load_dataset(\"allenai/multi_lexsum\", name=\"v20230518\")\n",
    "modified_dataset = multi_lexsum[\"test\"].filter(lambda x: x[\"summary/short\"] != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-3.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The plaintiffs filed a lawsuit on March 8, 2014, alleging that the City of Montgomery, Alabama, improperly imprisoned them for failing to pay traffic fines. They alleged that they did not have an ability to pay the fines due to their financial circumstances and that the city did not consider their ability to pay. On May 1, 2014, the District Court granted the plaintiffs motion for a preliminary injunction, preventing the city from collecting more money from traffic tickets of plaintiffs'. On October 31, 2014 the parties filed to dismiss the case pursuant to a settlement agreement, which included numerous changes to Municipal Court proceedings. The case is now closed.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_dataset[\"summary/short\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "# user_prompt = \"Summarize concisely the following legal texts. Include as many relevant facts as possible. A fact is relevant if it mentions plaintiffs, counsel, type of action, filling date, name of the court, description of class, defendants, statuatory basis, rought remedy, judges, consolidated class, whether it is a class action, date of decree, citations, duration of decrees, last action in case.\"\n",
    "system_prompt = \"You are a legal expert. You must answer concisely and truthfully, including only information that is relevant to the conversation. Stay faithful to the original text and keep the exact wording as found in the text as closely as possible. Only include facts relevant to the text, without any filler words.\"\n",
    "\n",
    "max_context_length = 8192\n",
    "max_output_length = 130\n",
    "length_system_prompt = len(tokenizer.encode(system_prompt))\n",
    "print(length_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from env_utils import load_env_from_file\n",
    "from groq import Groq\n",
    "\n",
    "load_env_from_file(\".\")\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_prompt_summary_basic = \"Summarize the text:<INPUT>{INPUT}</INPUT>\\nSummary:\"\n",
    "user_prompt_summary_detailed = \"Imagine you're a legal scholar tasked with distilling the essence of a complex case law into a concise and compelling summary. Your summary must encompass a rich tapestry of legal elements, including the identities of the plaintiffs and defendants, the brilliant minds behind the legal arguments (counsel), the decisive actions taken, the chronological tapestry of dates, the distinguished court presiding over the matter, the statutory framework underpinning the case, the sought-after remedy, the esteemed judges imparting wisdom, any case consolidations adding layers of complexity, the potential for class action ramifications, the pivotal date of decrees shaping the outcome, the temporal span of decree effectiveness, the authoritative citations grounding the legal analysis, and the climactic last action in the case. Your summary should weave these elements together seamlessly, ensuring a vivid and comprehensive portrayal of the legal landscape: <INPUT>{INPUT}</INPUT>\\nSummary:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Errors=0: 100%|██████████| 2/2 [00:00<00:00, 6091.94it/s]\n",
      "Errors=0: 100%|██████████| 2/2 [00:00<00:00, 8551.08it/s]\n",
      "Errors=0: 100%|██████████| 2/2 [00:00<00:00, 14979.66it/s]\n",
      "Errors=0: 100%|██████████| 2/2 [00:00<00:00, 8738.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from groq_functions import *\n",
    "llm = llmResponse(\"openai\", \"basic\", 2)\n",
    "llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename=\"test.log\", encoding=\"utf-8\", level=logging.WARNING, filemode=\"w\")\n",
    "\n",
    "logging.error(\"bro??\")\n",
    "logging.error(\"bro??\")\n",
    "logging.error(\"bro??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Errors=7: 100%|██████████| 616/616 [12:41<00:00,  1.24s/it] \n",
      "Errors=2: 100%|██████████| 616/616 [03:38<00:00,  2.82it/s]  \n",
      "Errors=0: 100%|██████████| 616/616 [00:00<00:00, 3667.89it/s]\n",
      "Errors=0: 100%|██████████| 616/616 [00:00<00:00, 3745.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential, RetryError\n",
    "\n",
    "def from_extracted(path, test_size, current_length, sentences=2):\n",
    "    summs = []\n",
    "    files = os.listdir(path)\n",
    "    files = sorted(files, key = lambda x: int(x.split(\".\")[0]))\n",
    "    for file in files:\n",
    "        if int(file.split(\".\")[0]) < test_size:\n",
    "            docs = json.load(open(path + file, \"r\"))\n",
    "\n",
    "            doc = \"\".join([\"\".join(doc_sentences[:sentences]) for doc_sentences in docs])\n",
    "            if len(tokenizer.encode(doc)) + current_length > max_context_length:\n",
    "                doc = \"\"\n",
    "                sent_limit = sentences-1\n",
    "                while (doc == \"\" and len(doc) + current_length < max_context_length) and sent_limit > 0:\n",
    "                    doc = \"\".join([\"\".join(doc_sentences[:sent_limit]) for doc_sentences in docs])\n",
    "                    sent_limit -= 1\n",
    "\n",
    "            summs.append(doc)\n",
    "\n",
    "    return summs\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=10), stop=stop_after_attempt(3))\n",
    "def completion_with_retry(file_path, **kwargs):\n",
    "    # file_path: model/prompt_type/extract_sum_type/idx\n",
    "    model, prompt_type, extract_sum_type, idx = file_path.split(\"/\")[1:]\n",
    "    success = False\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(**kwargs)\n",
    "        json.dump([*message_history, {\"role\": \"assistant\", \"content\": chat_completion.choices[0].message.content}], open(f\"{file_path}.json\", \"w\"), indent=2)  \n",
    "        success = True  \n",
    "    except Exception as exc:\n",
    "        logging.error(f\"{type(exc).__name__} ({exc.args}) - {prompt_type} - {extract_sum_type} - {idx}\")\n",
    "        success = False  \n",
    "\n",
    "    return success\n",
    "    \n",
    "\n",
    "test_size = 616\n",
    "extract_types = np.asarray([\"random_selection\", \"first5last5\", \"random_selection_bert\", \"first5last5_bert\"])\n",
    "model = \"gemma-7b-it\"\n",
    "user_prompt_length = len(tokenizer.encode(user_prompt_summary_detailed))\n",
    "prompt_type = \"detailed\"\n",
    "\n",
    "if model not in os.listdir(\"answers\"):\n",
    "    os.mkdir(f\"answers/{model}\")\n",
    "\n",
    "if prompt_type not in os.listdir(f\"answers/{model}\"):\n",
    "    os.mkdir(f\"answers/{model}/{prompt_type}\")\n",
    "\n",
    "logger = logging.getLogger(model)\n",
    "logging.basicConfig(filename=f\"log_{model}.log\", encoding=\"utf-8\", level=logging.WARNING)\n",
    "for extract_sum_type in extract_types:\n",
    "    responses = []\n",
    "    basic_responses = []\n",
    "    path = f\"extracted_sums/extracted_sums_json_{extract_sum_type}/\"\n",
    "    if extract_sum_type not in os.listdir(f\"answers/{model}/{prompt_type}/\"):\n",
    "        os.mkdir(f\"answers/{model}/{prompt_type}/{extract_sum_type}/\")\n",
    "\n",
    "    extracted_summaries = from_extracted(path, test_size=test_size, sentences=2, current_length=max_output_length+user_prompt_length+length_system_prompt)\n",
    "    errors = 0\n",
    "    iterator = tqdm(extracted_summaries[:test_size], desc=f\"Errors={errors}\")\n",
    "\n",
    "    for idx, summ in enumerate(iterator):\n",
    "        if f\"{idx}.json\" in os.listdir(f\"answers/{model}/{prompt_type}/{extract_sum_type}\"):\n",
    "            continue\n",
    "        basic_prompt = user_prompt_summary_detailed.format(INPUT=summ)\n",
    "        message_history = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": basic_prompt\n",
    "                }\n",
    "            ]\n",
    "        chat_completion_success = completion_with_retry(\n",
    "            file_path=f\"answers/{model}/{prompt_type}/{extract_sum_type}/{idx}\",\n",
    "            messages=message_history,\n",
    "            model=model,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        if not chat_completion_success:\n",
    "            errors += 1\n",
    "            iterator.set_description(f\"Errors={errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ['led: rouge1: 45.89', 'led: rouge2: 23.00', 'led: rougeL: 31.17', 'led: rougeLsum: 32.01']\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ['primera: rouge1: 42.87', 'primera: rouge2: 20.79', 'primera: rougeL: 29.31', 'primera: rougeLsum: 29.79']\u001b[39;00m\n\u001b[1;32m      6\u001b[0m rouge_scoring \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrouge_scoring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodified_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msummary/short\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_stemmer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/groq_summary/lib/python3.11/site-packages/evaluate/module.py:450\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/groq_summary/lib/python3.11/site-packages/evaluate/module.py:509\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m batch \u001b[38;5;241m=\u001b[39m {input_name: batch[input_name] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_feature_from_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_writer()\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/groq_summary/lib/python3.11/site-packages/evaluate/module.py:590\u001b[0m, in \u001b[0;36mEvaluationModule._infer_feature_from_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 590\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_example(example)\n",
      "File \u001b[0;32m~/anaconda3/envs/groq_summary/lib/python3.11/site-packages/evaluate/module.py:590\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 590\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k, \u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_feature_from_example(example)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# ['led: rouge1: 45.89', 'led: rouge2: 23.00', 'led: rougeL: 31.17', 'led: rougeLsum: 32.01']\n",
    "# ['primera: rouge1: 42.87', 'primera: rouge2: 20.79', 'primera: rougeL: 29.31', 'primera: rougeLsum: 29.79']\n",
    "\n",
    "rouge_scoring = evaluate.load(\"rouge\")\n",
    "print(rouge_scoring.compute(predictions=responses, references=modified_dataset[:test_size][\"summary/short\"], use_stemmer = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple-random: {'rouge1': 0.3669443053395881, 'rouge2': 0.12299475909651572, 'rougeL': 0.2148706258231231, 'rougeLsum': 0.22091825483890215}\n",
    "# simple-5/5: {'rouge1': 0.3600777579472476, 'rouge2': 0.11723337466920167, 'rougeL': 0.21617088924165473, 'rougeLsum': 0.22142449133012626}\n",
    "# simple-randombert: {'rouge1': 0.3542897797369077, 'rouge2': 0.10311088709231235, 'rougeL': 0.2069714451531819, 'rougeLsum': 0.20992633649991677}\n",
    "# simple-5/5bert: {'rouge1': 0.35138148214931236, 'rouge2': 0.104968022054537, 'rougeL': 0.20538624503075653, 'rougeLsum': 0.20893048554674404}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facilex_caselaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
