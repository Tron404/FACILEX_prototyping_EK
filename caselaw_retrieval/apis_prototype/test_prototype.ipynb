{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# assumption that data contains non-relevant elements of code/URL\n",
    "def sanitize_data(text: str) -> str:\n",
    "    text = re.sub(r\"(?:https://)?www.[^\\s<]+\", \"\", text) # remove potential websites\n",
    "    text = re.sub(r\"<.*?>\", \"\", text) # remove HTML elements\n",
    "    text = re.sub(r\"&nbsp;\", \"\", text) # remove leftover formatting\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# given the embedded representation of a query case and the embeddings of the cases through which to search, alongside their respective CELEX IDs,\n",
    "# return the ordered indices (from most similar to least) of retrieved cases\n",
    "def cosine_similarity_search(query: np.ndarray, search_space: np.ndarray, query_celex: str, search_space_celex: list, top_k: int = 5) -> list:\n",
    "    cosine_scores: list = cosine_similarity(query.reshape(1,-1), search_space)[0]\n",
    "\n",
    "    # sort the similarity scores from most to least similar and select the top 5 most similar cases alongside their CELEX IDs\n",
    "    all_matches: np.ndarray = np.argsort(cosine_scores)[::-1]\n",
    "    best_matches: np.ndarray = all_matches[:top_k]\n",
    "    search_space_celex: list = search_space_celex[best_matches]\n",
    "\n",
    "    # similar_cases: list = [] # indices of most similar cases\n",
    "    # for retrieved_case, retrieved_case_celex in zip(best_matches, search_space_celex):\n",
    "    #     if query_celex == retrieved_case_celex: # only show the user cases which do have the same CELEX number; this can be removed to include semantically similar cases which differ in their CELEX\n",
    "    #         similar_cases.append(retrieved_case)\n",
    "\n",
    "    # list comprehension faster than normal looping\n",
    "    similar_cases: list = [retrieved_case for retrieved_case, retrieved_case_celex in zip(best_matches, search_space_celex) if query_celex == retrieved_case_celex]\n",
    "\n",
    "    return similar_cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # for embedding using transformers\n",
    "    embedding_model: SentenceTransformer = SentenceTransformer(\"../../models/multi-qa-mpnet-base-dot-v1\", device = device).half() # fp16\n",
    "\n",
    "    query_data: dict = json.load(open(\"example_input_query.json\", \"r\")) # json of case that facilitates query\n",
    "    search_data: dict = json.load(open(\"corpus.json\", \"r\")) # json of all available cases on which to search for similar ones\n",
    "\n",
    "    query_data[\"summaryEn\"] = sanitize_data(query_data[\"summaryEn\"])\n",
    "    for search_text in search_data:\n",
    "        search_text[\"summaryEn\"] = sanitize_data(search_text[\"summaryEn\"])\n",
    "\n",
    "    # extract the most important of information from the search json and store it in lists to later convert into json items\n",
    "    # list comprehension better than normal looping\n",
    "    # search_celex = []\n",
    "    # search_texts = []\n",
    "    # search_jurisdiction = []\n",
    "    # for search_space_item in search_data:\n",
    "    #     search_celex.append(search_space_item[\"euProvisions\"])\n",
    "    #     search_texts.append(search_space_item[\"summaryEn\"])\n",
    "    #     search_jurisdiction.append(search_space_item[\"jurisdiction\"])\n",
    "\n",
    "    search_celex = [search_space_item[\"euProvisions\"] for search_space_item in search_data]\n",
    "    search_texts = [search_space_item[\"summaryEn\"] for search_space_item in search_data]\n",
    "    search_jurisdiction = [search_space_item[\"jurisdiction\"] for search_space_item in search_data]\n",
    "\n",
    "    search_embedded: np.ndarray = embedding_model.encode(search_texts)\n",
    "    query_embedded: np.ndarray = embedding_model.encode(query_data[\"summaryEn\"])\n",
    "    # convert lists to np.ndarray to facilitate index slicing\n",
    "    search_celex = np.asarray(search_celex)\n",
    "    search_embedded = np.asarray(search_embedded)\n",
    "    search_jurisdiction = np.asarray(search_jurisdiction)\n",
    "\n",
    "    recommended_idx: list = cosine_similarity_search(query_embedded, search_embedded, query_data[\"euProvisions\"], search_celex)\n",
    "    output_data = [{\"summaryEn\": search_texts[idx], \"euProvisions\": search_celex[idx], \"jurisdiction\": search_jurisdiction[idx]} for idx in recommended_idx]\n",
    "\n",
    "    json.dump(output_data, open(\"example_output.json\", \"w\"))\n",
    "\n",
    "    del embedding_model    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facilex_caselaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
