{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summaryEn</th>\n",
       "      <th>euCaselaw</th>\n",
       "      <th>jurisdiction</th>\n",
       "      <th>celex</th>\n",
       "      <th>citation_all</th>\n",
       "      <th>citation_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VSRH, Kž eun 27/2017-4</td>\n",
       "      <td>The case concerns the crime of fraud committed...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>{32002F0584}</td>\n",
       "      <td>{32002F0584.Article_8.Paragraph_1.Point_c}</td>\n",
       "      <td>{32002F0584.Article_8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rechtbank Amsterdam, 11-06-2020, ECLI:NL:RBAMS...</td>\n",
       "      <td>The case concerns the crime of [assault.] prov...</td>\n",
       "      <td>[{'celex': '62016CJ0367', 'name': 'Judgment of...</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>{32002F0584}</td>\n",
       "      <td>{32002F0584.Article_2.Paragraph_1, 32002F0584....</td>\n",
       "      <td>{32002F0584.Article_2, 32002F0584.Article_3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wyrok Sądu Najwyższego z dnia 4 lipca 2013 r. ...</td>\n",
       "      <td>The application of detention on remand in the ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Poland</td>\n",
       "      <td>{32002F0584}</td>\n",
       "      <td>{32002F0584.Article_27}</td>\n",
       "      <td>{32002F0584.Article_27}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rechtbank Amsterdam, 14-09-2023, ECLI:NL:RBAMS...</td>\n",
       "      <td>The case concerns the crime of [unknown] provi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>{32018R1805}</td>\n",
       "      <td>{32018R1805.Article_8, 32018R1805.Article_4.Pa...</td>\n",
       "      <td>{32018R1805.Article_4, 32018R1805.Article_8}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Juzgado Central de Instrucción núm. 4. Auto 88...</td>\n",
       "      <td>The case concerns the crimes of The case conce...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Spain</td>\n",
       "      <td>{32002F0584}</td>\n",
       "      <td>{32002F0584}</td>\n",
       "      <td>{32002F0584}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                             VSRH, Kž eun 27/2017-4   \n",
       "1  Rechtbank Amsterdam, 11-06-2020, ECLI:NL:RBAMS...   \n",
       "2  Wyrok Sądu Najwyższego z dnia 4 lipca 2013 r. ...   \n",
       "3  Rechtbank Amsterdam, 14-09-2023, ECLI:NL:RBAMS...   \n",
       "4  Juzgado Central de Instrucción núm. 4. Auto 88...   \n",
       "\n",
       "                                           summaryEn  \\\n",
       "0  The case concerns the crime of fraud committed...   \n",
       "1  The case concerns the crime of [assault.] prov...   \n",
       "2  The application of detention on remand in the ...   \n",
       "3  The case concerns the crime of [unknown] provi...   \n",
       "4  The case concerns the crimes of The case conce...   \n",
       "\n",
       "                                           euCaselaw jurisdiction  \\\n",
       "0                                                 []      Croatia   \n",
       "1  [{'celex': '62016CJ0367', 'name': 'Judgment of...  Netherlands   \n",
       "2                                                 []       Poland   \n",
       "3                                                 []  Netherlands   \n",
       "4                                                 []        Spain   \n",
       "\n",
       "          celex                                       citation_all  \\\n",
       "0  {32002F0584}         {32002F0584.Article_8.Paragraph_1.Point_c}   \n",
       "1  {32002F0584}  {32002F0584.Article_2.Paragraph_1, 32002F0584....   \n",
       "2  {32002F0584}                            {32002F0584.Article_27}   \n",
       "3  {32018R1805}  {32018R1805.Article_8, 32018R1805.Article_4.Pa...   \n",
       "4  {32002F0584}                                       {32002F0584}   \n",
       "\n",
       "                               citation_article  \n",
       "0                        {32002F0584.Article_8}  \n",
       "1  {32002F0584.Article_2, 32002F0584.Article_3}  \n",
       "2                       {32002F0584.Article_27}  \n",
       "3  {32018R1805.Article_4, 32018R1805.Article_8}  \n",
       "4                                  {32002F0584}  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "path_data = \"../data/caselaw_data/\"\n",
    "\n",
    "main_attributes = json.load(open(path_data + \"2301.json\", \"rb\"))\n",
    "os.mkdir(\"../data/caselaw_emb/\") if \"caselaw_emb\" not in os.listdir(\"../data/\") else 1\n",
    "\n",
    "data = {key: [] for key in main_attributes}\n",
    "\n",
    "for case in os.listdir(path_data):\n",
    "    file = json.load(open(path_data + case, \"rb\"))\n",
    "    for attribute in data.keys():\n",
    "        data[attribute].append(file[attribute])\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "# select only relevant data and drop any data points that have no EU provisions mentioned\n",
    "relevant_cols = [\"title\", \"summaryEn\", \"euCaselaw\", \"euProvisions\", \"jurisdiction\"]\n",
    "df = df[relevant_cols]\n",
    "df = df[(df[\"euProvisions\"].str.len() > 0)]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "def get_citation_to_eu_instrument(item):\n",
    "    complete_citation = [mention[\"label\"] + \"_\" + mention[\"value\"] for mention in item[\"itemsBase\"] if mention[\"value\"] != \"\"]\n",
    "    return item[\"celex\"] + \".\" + \".\".join(complete_citation) if len(complete_citation) > 0 else item[\"celex\"]\n",
    "\n",
    "# denest the jurisdiction, CELEX, and EU citations\n",
    "df[\"jurisdiction\"] = df[\"jurisdiction\"].apply(lambda x: x[\"label\"])\n",
    "df[\"celex\"] = df[\"euProvisions\"].apply(lambda x: set([citation[\"celex\"] for citation in x]))\n",
    "df[\"citation_all\"] = df[\"euProvisions\"].apply(lambda x: set([get_citation_to_eu_instrument(citation) for citation in x]))\n",
    "df[\"citation_article\"] = df[\"euProvisions\"].apply(lambda x: set([f\"{citation['celex']}.{citation['itemsBase'][0]['label']}_{citation['itemsBase'][0]['value']}\" if citation['itemsBase'][0]['value'] != \"\" else citation['celex']  for citation in x]))\n",
    "\n",
    "# drop EU provisions, as all information was extracted from it\n",
    "df = df.drop([\"euProvisions\"], axis = 1)\n",
    "\n",
    "# sanitise summaries\n",
    "df[\"summaryEn\"] = df[\"summaryEn\"].apply(lambda x: re.sub(r\"(?:https://)?www.[^\\s<]+\", \"\", x)) # remove any potential links\n",
    "df[\"summaryEn\"] = df[\"summaryEn\"].apply(lambda x: re.sub(r\"<.*?>\", \"\", x)) # remove html elements\n",
    "df[\"summaryEn\"] = df[\"summaryEn\"].apply(lambda x: re.sub(r\"&nbsp;\", \"\", x)) # remove html elements\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r ../data/caselaw_emb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_embeddings(data, tokenized, pad_tok_id):\n",
    "  if \"attention_mask\" in tokenized:\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "  else: # apparently ErnieM does NOT have attenion IDs in the tokenized output, so I am \"computing\" them myself - like in all other models, the model should not pay attention to [PAD] tokens, so they are ignored/not paid attention to\n",
    "    print(\"Oh no\")\n",
    "    token_ids = tokenized[\"input_ids\"][0]\n",
    "    padding_ids = len([tok for tok in token_ids if tok == pad_tok_id]) # count how many [PAD] tokens there are\n",
    "    attention_mask = torch.ones((tokenized[\"input_ids\"].shape)).to(device)\n",
    "    if padding_ids > 0:\n",
    "        attention_mask[:,-padding_ids:] = 0\n",
    "    attention_mask = torch.tensor(attention_mask).to(device)\n",
    "    \n",
    "  attention_expanded = attention_mask.unsqueeze(-1).expand(data.size()).float()\n",
    "  data_attention = data * attention_expanded\n",
    "  return torch.sum(data_attention, 1) / torch.clamp(attention_expanded.sum(1), min=1e-9) # to not divide by 0\n",
    "\n",
    "def encode_sentence_bert(text, model, tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\", return_attention_mask=True)\n",
    "    \n",
    "    pad_tok_id = tokenizer(\"[PAD]\")\n",
    "    pad_tok_id = pad_tok_id[\"input_ids\"][1]\n",
    "    \n",
    "    aux = {}\n",
    "    for key in inputs.keys(): # cast tokenized input to GPU\n",
    "      aux[key] = inputs[key].to(device)\n",
    "    inputs = aux\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    outputs = pool_embeddings(outputs[0], inputs, pad_tok_id)\n",
    "\n",
    "    last_hidden_state = outputs.cpu().detach().numpy()  # The last hidden-state is the first element of the output tuple\n",
    "\n",
    "    return last_hidden_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keddie/anaconda3/envs/facilex_caselaw/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import fasttext\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_sentence_embedding_fasttext(tokens, model):\n",
    "    aux = []\n",
    "    for token in tokens:\n",
    "        aux.append(model.get_word_vector(token))\n",
    "    \n",
    "    aux = np.asarray(aux)\n",
    "\n",
    "    return np.mean(aux, axis = 0)\n",
    "\n",
    "\n",
    "def get_embedding_all_fasttext(data, model):\n",
    "    match model:\n",
    "        case \"facilex\":\n",
    "            if \"fasttext_facilex.bin\" in os.listdir(\"../models/\"):\n",
    "                fasttext_model = fasttext.load_model(f\"../models/fasttext_{model}.bin\")\n",
    "            else:\n",
    "                fasttext_model = train_fasttext()\n",
    "        case _:\n",
    "            fasttext_model = fasttext.load_model(f\"../models/{model}.bin\")\n",
    "            \n",
    "    data[\"summaryEn\"] = data[\"summaryEn\"].apply(summary_preprocessing)\n",
    "    data[\"summaryEn\"] = data[\"summaryEn\"].str.split(\" \")\n",
    "    all_vectors = data[\"summaryEn\"].apply(get_sentence_embedding_fasttext, args = (fasttext_model, ))        \n",
    "\n",
    "    return np.asarray(all_vectors.tolist())\n",
    "\n",
    "def train_fasttext():\n",
    "    with open(\"processed_text_fasttext.txt\", \"w\") as file:\n",
    "        for text in df[\"summaryEn\"].apply(summary_preprocessing):\n",
    "            file.write(text)\n",
    "            file.write(\"\\n\")\n",
    "    fasttext_model = fasttext.train_unsupervised(\"processed_text_fasttext.txt\", dim = 500, epoch = 20)\n",
    "    fasttext_model.save_model(\"../models/fasttext_facilex.bin\")\n",
    "\n",
    "    return fasttext_model\n",
    "\n",
    "def summary_preprocessing(text):\n",
    "    text = re.sub(r\"[^\\w ]+\", \" \", text)\n",
    "    text = re.sub(r\" {2,}\", \" \", text)\n",
    "    text = text.strip(string.punctuation)\n",
    "    text = text.lower()\n",
    "    text = lemmatizer(text)\n",
    "    text = [token.lemma_ for token in text]\n",
    "    text = \" \".join([word for word in text if word not in stop_words])\n",
    "\n",
    "    return text\n",
    "\n",
    "def summary_preprocessing_transformer(text):\n",
    "    text = re.sub(r\"[^\\w ]+\", \" \", text)\n",
    "    text = re.sub(r\" {2,}\", \" \", text)\n",
    "    text = text.strip(string.punctuation)\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "all_tfidf = TfidfVectorizer(binary = True, ngram_range = (1,2), tokenizer = word_tokenize, lowercase = True)\n",
    "all_tfidf.fit(np.asarray(df[\"summaryEn\"].apply(summary_preprocessing).tolist()))\n",
    "\n",
    "def get_sentence_embedding_tfidf(data):\n",
    "    data[\"summaryEn\"] = data[\"summaryEn\"].apply(summary_preprocessing)\n",
    "    all_vectors = np.asarray(all_tfidf.transform(np.asarray(data[\"summaryEn\"].tolist())).toarray())\n",
    "\n",
    "    return all_vectors\n",
    "\n",
    "# train_fasttext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r ../data/caselaw_emb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6680/3048227423.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"summaryEn\"] = data[\"summaryEn\"].apply(summary_preprocessing)\n",
      "100%|██████████| 8/8 [00:07<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "embd_name_map = {\n",
    "    frozenset([get_sentence_embedding_tfidf]): \"tfidf\", \n",
    "    frozenset([get_embedding_all_fasttext, \"facilex\"]): \"fasttext_facilex\",\n",
    "    frozenset([get_embedding_all_fasttext, \"cc.en.300\"]): \"fasttext_ccen\"\n",
    "}\n",
    "\n",
    "for jurisdiction in tqdm(df[\"jurisdiction\"].unique()):\n",
    "    for embedding_method in [get_sentence_embedding_tfidf, [get_embedding_all_fasttext, \"facilex\"], [get_embedding_all_fasttext, \"cc.en.300\"]][:1]:\n",
    "        aux = df.copy(True)\n",
    "        if not type(embedding_method) == list:\n",
    "            emb_vec = embedding_method(aux[aux[\"jurisdiction\"] == jurisdiction])\n",
    "        else:\n",
    "            emb_vec = embedding_method[0](aux[aux[\"jurisdiction\"] == jurisdiction], embedding_method[1])\n",
    "\n",
    "        os.mkdir(f\"../data/caselaw_emb/{jurisdiction}\") if jurisdiction not in os.listdir(\"../data/caselaw_emb/\") else 1\n",
    "        pickle.dump(emb_vec, open(f\"../data/caselaw_emb/{jurisdiction}/emb_{embd_name_map[frozenset([embedding_method] if type(embedding_method) != list else embedding_method)]}.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]/home/keddie/anaconda3/envs/facilex_caselaw/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "100%|██████████| 8/8 [00:53<00:00,  6.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# models = [\"distiluse-base-multilingual-cased-v2\", \"paraphrase-multilingual-mpnet-base-v2\", \"bert-base-uncased\", \"multi-qa-mpnet-base-dot-v1\", \"all-mpnet-base-v2\"]\n",
    "models = [\"distiluse-base-multilingual-cased-v2\", \"paraphrase-multilingual-mpnet-base-v2\", \"multi-qa-mpnet-base-dot-v1\", \"all-mpnet-base-v2\"]\n",
    "\n",
    "model_name_map = {\n",
    "    \"distiluse-base-multilingual-cased-v2\": \"multi_distiluse\", \n",
    "    \"paraphrase-multilingual-mpnet-base-v2\": \"multi_mpnet\", \n",
    "    \"bert-base-uncased\": \"bert_uncased\", \n",
    "    \"multi-qa-mpnet-base-dot-v1\": \"multiqa_mpnet_dot\", \n",
    "    \"all-mpnet-base-v2\": \"mpnet\"\n",
    "}\n",
    "\n",
    "for jurisdiction in tqdm(df[\"jurisdiction\"].unique()):\n",
    "    for model_name in models:\n",
    "        aux = df.copy(True)\n",
    "        aux[\"summaryEn\"] = aux[\"summaryEn\"].apply(summary_preprocessing_transformer)\n",
    "        if \"bert\" not in model_name:\n",
    "            model = SentenceTransformer(\"../models/\" + model_name).to(device)\n",
    "            emb_vec = np.asarray(aux[aux[\"jurisdiction\"] == jurisdiction][\"summaryEn\"].apply(model.encode).tolist())\n",
    "        else:\n",
    "            model = BertModel.from_pretrained(\"../models/\" + model_name).to(device)\n",
    "            tokenizer = BertTokenizer.from_pretrained(\"../models/\" + model_name)\n",
    "\n",
    "            emb_vec = np.asarray(aux[aux[\"jurisdiction\"] == jurisdiction][\"summaryEn\"].apply(encode_sentence_bert, args = (model, tokenizer,)).tolist())\n",
    "\n",
    "        os.mkdir(f\"../data/caselaw_emb/{jurisdiction}\") if jurisdiction not in os.listdir(\"../data/caselaw_emb/\") else 1\n",
    "        aux_df = df[df[\"jurisdiction\"] == jurisdiction].copy(True)\n",
    "        aux_df[f\"embedding_{model_name}\"] = list(emb_vec)\n",
    "        aux_df.to_pickle(f\"../data/caselaw_emb/{jurisdiction}/emb_{model_name_map[model_name]}.pickle\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facilex_caselaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
