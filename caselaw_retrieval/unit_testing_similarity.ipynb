{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(317, 31)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Latex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from similarity_summary_eng import read_data, cosine_search_output, prepare_search\n",
    "from similarity_summary_eng import evaluate as eval_orig\n",
    "\n",
    "df_all = read_data()\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for ease of reading\n",
    "# https://stackoverflow.com/questions/39473297/how-do-i-print-colored-output-with-python-3\n",
    "from typing import Any\n",
    "\n",
    "class color_printing():\n",
    "    END = \"\\x1b[0m\"\n",
    "    Italic = \"\\x1b[3m\"\n",
    "    # If Foreground is False that means color effect on Background\n",
    "\n",
    "    def RGB(R, G, B, Foreground = True): # R: 0-255  ,  G: 0-255  ,  B: 0-255\n",
    "        FB_G = 38 # Effect on foreground\n",
    "        if Foreground != True:\n",
    "            FB_G = 48 # Effect on background\n",
    "        return \"\\x1b[\" + str(FB_G) + \";2;\" + str(R) + \";\" + str(G) + \";\" + str(B) + \"m\"\n",
    "    \n",
    "    def print_string(text, colours):\n",
    "        return color_printing.RGB(*colours) + text + color_printing.END\n",
    "    \n",
    "class exampleCalculation():\n",
    "    def evaluate(self, retrieval_scores, missed_retrieval_scores, top_k):\n",
    "        metrics = {}\n",
    "\n",
    "        #### precision \n",
    "        metrics[f\"precision@{top_k}\"] = round(np.mean(np.sum(retrieval_scores, axis = 1)/top_k, 0), 3)\n",
    "\n",
    "        #### mean average precision for a retrieval window of k = 5\n",
    "        average_precision = np.sum([(np.sum(retrieval_scores[:, :k], axis = 1)/k) * retrieval_scores[:, k-1] for k in range(1, top_k+1)])/top_k\n",
    "        metrics[f\"map@{top_k}\"] = round(average_precision/len(retrieval_scores), 3)\n",
    "        \n",
    "        #### MRR and MRR missed\n",
    "        aux_retrieval_scores = np.hstack([np.zeros((retrieval_scores.shape[0], 1)), retrieval_scores])\n",
    "        rank = np.argmax(aux_retrieval_scores, axis = 1) # get first non-zero rank\n",
    "        missed = np.min([top_k - np.count_nonzero(retrieval_scores, axis = 1), np.sum(missed_retrieval_scores, axis = 1)], axis = 0)\n",
    "        running_mrr = np.sum(np.divide(1, rank, where = rank != 0)) # rank starts at 0 bcs. of this so rank + 1\n",
    "        running_mrr_miss = np.sum(np.divide(1, rank + missed, where = rank != 0)) # rank starts at 0 bcs. of this so rank + 1\n",
    "        metrics[f\"mrr@{top_k}\"] = round(running_mrr/(len(retrieval_scores)), 3)\n",
    "        metrics[f\"mrr_miss@{top_k}\"] = round(running_mrr_miss/len(retrieval_scores), 3)\n",
    "\n",
    "        #### hitrate\n",
    "        metrics[f\"hitrate@{top_k}\"] = round(np.sum(np.any(retrieval_scores, axis = 1))/len(retrieval_scores), 3)\n",
    "\n",
    "        # display of formula computations to facilitate understanding\n",
    "        #### precision\n",
    "        running_precision_latex = [r\"\\frac{%d}{%d}\" % (np.sum(retrieved), top_k) for retrieved in retrieval_scores]\n",
    "        display(Latex(\"Precision@%d $= \\\\frac{\\\\#\\\\text{relevant retrieved cases}}{\\\\#\\\\text{total retrieved cases}} = \" % (top_k) + \"\\\\frac{{1}}{%d}\\cdot(%s)\" % (len(running_precision_latex), '+'.join(running_precision_latex)) + f\"={metrics[f'precision@{top_k}']}$\"))\n",
    "\n",
    "        #### MRR\n",
    "        sum_mrr_latex = [r'\\frac{%d}{%d}' % (1, np.argmax(np.hstack([[0], retrieved]))) for retrieved in retrieval_scores]\n",
    "        running_mrr_latex = \"\\\\frac{{1}}{%d}\\cdot(%s)\" % (retrieval_scores.shape[0], '+'.join(sum_mrr_latex))\n",
    "        display(Latex(\"MRR@%d $= \\\\frac{1}{\\\\text{\\\\# queries}}\\\\sum_q^Q \\\\frac{1}{rank_q}=\" % (top_k) + running_mrr_latex + f\"= {metrics[f'mrr@{top_k}']}$\"))\n",
    "        \n",
    "        ### MRR miss\n",
    "        sum_mrr_miss_latex = [r'\\frac{%d}{%d+%d}' % (1, np.argmax(np.hstack([[0], retrieved])), np.min([top_k - np.count_nonzero(retrieved), np.sum(missed)])) for retrieved, missed in zip(retrieval_scores, missed_retrieval_scores)]\n",
    "        running_mrr_miss_latex = \"\\\\frac{{1}}{%d}\\cdot(%s)\" % (retrieval_scores.shape[0], '+'.join(sum_mrr_miss_latex))\n",
    "        display(Latex(\"MRR missed@%d $= \\\\frac{1}{\\\\text{\\\\# queries}}\\\\sum_q^Q \\\\frac{1}{rank_q+miss_q}=\" % (top_k) + running_mrr_miss_latex + f\"= {metrics[f'mrr_miss@{top_k}']}$\"))\n",
    "\n",
    "        ### hitrate\n",
    "        running_hitrate_latex = [\"%d\" % (np.any(retrieved)) for retrieved in retrieval_scores]\n",
    "        display(Latex(\"Hitrate@%d $=\\\\frac{1}{\\\\text{\\\\# queries}}\\\\sum_q^Q any\\\\_rel(q)=\" % (top_k) + \"\\\\frac{{1}}{%d}\\cdot (%s)\" % (len(retrieval_scores), '+'.join(running_hitrate_latex)) + f\" = {metrics[f'hitrate@{top_k}']}$\"))\n",
    "\n",
    "        ### average precision\n",
    "        running_ap_latex = [\"[\" + \"+\".join(['\\\\frac{%d}{%d}\\cdot%d' % (np.sum(retrieved[:k]), k, retrieved[k-1]) for k in range(1, top_k+1)]) + \"]\" for retrieved in retrieval_scores]\n",
    "        running_ap_latex_aux_calculations = np.round([[np.sum(retrieved[:k])/k*retrieved[k-1] for k in range(1, top_k+1)] for retrieved in retrieval_scores], 3)\n",
    "        display(Latex(\"MAP@%d $ = \\\\frac{1}{\\\\text{\\\\# queries}}\\\\sum_q^Q \\\\frac{1}{\\\\text{top\\\\_k}}\\sum_{k=1}^{\\\\text{top\\\\_k}}P_q(k)\\\\cdot rel_q(k)=\" % (top_k) + \"\\\\frac{{1}}{%d}(%s)\" % (len(retrieval_scores), '+'.join(running_ap_latex)) + \"= \\\\frac{{1}}{%d}\" % (len(retrieval_scores))+ f\"({'+'.join(np.sum(running_ap_latex_aux_calculations, axis = 1).astype(str))}) = {metrics[f'map@{top_k}']}$\"))\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def example(self, sample_case, search_corpus, query_type, top_k, model):\n",
    "        # name mapping to disambiguate what the system used as the ground truth\n",
    "        print(top_k)\n",
    "        query_map = {\n",
    "            \"celex\": \"CELEX ID\",\n",
    "            \"citation_article\": \"article-only EU citation\"\n",
    "        }\n",
    "\n",
    "        if len(sample_case) < 2:\n",
    "            cosine_scores = cosine_similarity(np.asarray(sample_case[model].tolist()).reshape(1,-1), np.asarray(search_corpus[model].tolist()))\n",
    "        else:\n",
    "            cosine_scores = cosine_similarity(np.asarray(sample_case[model].tolist()), np.asarray(search_corpus[model].tolist()))\n",
    "\n",
    "        retrieval_scores = []\n",
    "        missed_retrieval_scores = []\n",
    "\n",
    "        for idx in range(sample_case.shape[0]):\n",
    "            all_matches = np.argsort(cosine_scores[idx])[::-1]\n",
    "\n",
    "            best_matches = all_matches[1:top_k+1]\n",
    "            query_celex = sample_case.iloc[idx][query_type]\n",
    "\n",
    "            print(f\"CELEX ID of query case = {color_printing.print_string(str(query_celex), colours = (255, 150, 0))}\")\n",
    "            print(\"---------------\")\n",
    "\n",
    "            # see which top_k retrieved documents were retrieved\n",
    "            score = []\n",
    "            for retrieved_case in best_matches:\n",
    "                retrieved_celex = search_corpus.iloc[retrieved_case][query_type]\n",
    "                score.append(int(len(query_celex & retrieved_celex) > 0))\n",
    "\n",
    "                print(f\"CELEX IDs of retrieved case = {color_printing.print_string(str(retrieved_celex), colours = (255, 150, 0))} | They share at least one {color_printing.print_string(query_map[query_type], colours = (0, 150, 0))} = {color_printing.print_string(str(len(retrieved_celex & query_celex) > 0), colours = (255, 150, 0))} ===>> Retrieval score becomes = {color_printing.print_string(str(score), colours = (255, 150, 0))}\")\n",
    "\n",
    "            ### find all relevant cases that were missed\n",
    "            missed_matches = []\n",
    "            # for retrieved_case in all_matches[1:len(df_temp)]:\n",
    "            for retrieved_case in all_matches[1:]:\n",
    "                retrieved_celex = search_corpus.iloc[retrieved_case][query_type]\n",
    "                missed_matches.append(int(len(query_celex & retrieved_celex) > 0))\n",
    "\n",
    "            print(\"=====================\")\n",
    "            print(f\"Final retrieval score = {color_printing.print_string(str(score), colours = (255, 150, 0))} ==>> There are {color_printing.print_string(str(np.sum(score)), colours = (255, 150, 0))} retrieved cases which share the same {color_printing.print_string(query_map[query_type], colours = (0, 150, 0))} as the query case, out of k = {color_printing.print_string(str(top_k), colours = (255, 150, 0))}\")\n",
    "            print(f\"Number of cases relevant to the query, which are present in the entire dataset but not in the retrieval window = {color_printing.print_string(str(np.sum(missed_matches)), colours = (255, 150, 0))}; however, there are {color_printing.print_string(str(top_k - np.count_nonzero(score)), colours = (255, 150, 0))} retrieved non-relevant cases\")\n",
    "\n",
    "            score = np.asarray(score)\n",
    "            missed_matches = np.asarray(missed_matches)\n",
    "            retrieval_scores.append(score)\n",
    "            missed_retrieval_scores.append(missed_matches)\n",
    "\n",
    "        metrics = self.evaluate(np.asarray(retrieval_scores), np.asarray(missed_retrieval_scores), top_k = top_k)\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        self.example(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "CELEX ID of query case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m\n",
      "---------------\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 1]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 1, 1]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584.Article_4 bis'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mFalse\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 1, 1, 0]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 1, 1, 0, 1]\u001b[0m\n",
      "=====================\n",
      "Final retrieval score = \u001b[38;2;255;150;0m[1, 1, 1, 0, 1]\u001b[0m ==>> There are \u001b[38;2;255;150;0m4\u001b[0m retrieved cases which share the same \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m as the query case, out of k = \u001b[38;2;255;150;0m5\u001b[0m\n",
      "Number of cases relevant to the query, which are present in the entire dataset but not in the retrieval window = \u001b[38;2;255;150;0m6\u001b[0m; however, there are \u001b[38;2;255;150;0m1\u001b[0m retrieved non-relevant cases\n",
      "CELEX ID of query case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m\n",
      "---------------\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584.Article_4 bis'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mFalse\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 0]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 0, 1]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 0, 1, 1]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 0, 1, 1, 1]\u001b[0m\n",
      "=====================\n",
      "Final retrieval score = \u001b[38;2;255;150;0m[1, 0, 1, 1, 1]\u001b[0m ==>> There are \u001b[38;2;255;150;0m4\u001b[0m retrieved cases which share the same \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m as the query case, out of k = \u001b[38;2;255;150;0m5\u001b[0m\n",
      "Number of cases relevant to the query, which are present in the entire dataset but not in the retrieval window = \u001b[38;2;255;150;0m6\u001b[0m; however, there are \u001b[38;2;255;150;0m1\u001b[0m retrieved non-relevant cases\n",
      "CELEX ID of query case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m\n",
      "---------------\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 1]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 1, 1]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 1, 1, 1]\u001b[0m\n",
      "CELEX IDs of retrieved case = \u001b[38;2;255;150;0m{'32002F0584'}\u001b[0m | They share at least one \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m = \u001b[38;2;255;150;0mTrue\u001b[0m ===>> Retrieval score becomes = \u001b[38;2;255;150;0m[1, 1, 1, 1, 1]\u001b[0m\n",
      "=====================\n",
      "Final retrieval score = \u001b[38;2;255;150;0m[1, 1, 1, 1, 1]\u001b[0m ==>> There are \u001b[38;2;255;150;0m5\u001b[0m retrieved cases which share the same \u001b[38;2;0;150;0marticle-only EU citation\u001b[0m as the query case, out of k = \u001b[38;2;255;150;0m5\u001b[0m\n",
      "Number of cases relevant to the query, which are present in the entire dataset but not in the retrieval window = \u001b[38;2;255;150;0m6\u001b[0m; however, there are \u001b[38;2;255;150;0m0\u001b[0m retrieved non-relevant cases\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "Precision@5 $= \\frac{\\#\\text{relevant retrieved cases}}{\\#\\text{total retrieved cases}} = \\frac{{1}}{3}\\cdot(\\frac{4}{5}+\\frac{4}{5}+\\frac{5}{5})=0.867$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "MRR@5 $= \\frac{1}{\\text{\\# queries}}\\sum_q^Q \\frac{1}{rank_q}=\\frac{{1}}{3}\\cdot(\\frac{1}{1}+\\frac{1}{1}+\\frac{1}{1})= 1.0$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "MRR missed@5 $= \\frac{1}{\\text{\\# queries}}\\sum_q^Q \\frac{1}{rank_q+miss_q}=\\frac{{1}}{3}\\cdot(\\frac{1}{1+1}+\\frac{1}{1+1}+\\frac{1}{1+0})= 0.667$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "Hitrate@5 $=\\frac{1}{\\text{\\# queries}}\\sum_q^Q any\\_rel(q)=\\frac{{1}}{3}\\cdot (1+1+1) = 1.0$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "MAP@5 $ = \\frac{1}{\\text{\\# queries}}\\sum_q^Q \\frac{1}{\\text{top\\_k}}\\sum_{k=1}^{\\text{top\\_k}}P_q(k)\\cdot rel_q(k)=\\frac{{1}}{3}([\\frac{1}{1}\\cdot1+\\frac{2}{2}\\cdot1+\\frac{3}{3}\\cdot1+\\frac{3}{4}\\cdot0+\\frac{4}{5}\\cdot1]+[\\frac{1}{1}\\cdot1+\\frac{1}{2}\\cdot0+\\frac{2}{3}\\cdot1+\\frac{3}{4}\\cdot1+\\frac{4}{5}\\cdot1]+[\\frac{1}{1}\\cdot1+\\frac{2}{2}\\cdot1+\\frac{3}{3}\\cdot1+\\frac{4}{4}\\cdot1+\\frac{5}{5}\\cdot1])= \\frac{{1}}{3}(3.8+3.2169999999999996+5.0) = 0.801$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_calc = exampleCalculation()\n",
    "example_calc(df_all.iloc[7:10], df_all, \"citation_article\", 5, \"embedding_multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_retrieval_label (__main__.unitTestSimilarity.test_retrieval_label) ... ok\n",
      "test_similarity_order (__main__.unitTestSimilarity.test_similarity_order) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.038s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x70b06d183d50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class unitTestSimilarity(unittest.TestCase):\n",
    "    def __init__(self, methodName: str = \"runTest\") -> None:\n",
    "        super().__init__(methodName)\n",
    "        self._get_results()\n",
    "\n",
    "    def _get_results(self):\n",
    "        embds_query, embds_search = prepare_search(df_all, \"embedding_multi-qa-mpnet-base-dot-v1\")\n",
    "        self.retrieval_scores, self.missed_retrieval_scores, self.cosine_scores, self.idx_retrieved_cases = cosine_search_output(embds_query, embds_search, \"celex\", df_all, 5, 0, True)\n",
    "    \n",
    "    def test_similarity_order(self):\n",
    "        # check if similarity of retrieved scores is indeed from most similar to least\n",
    "        for (idx_query, idx_list) in self.idx_retrieved_cases:\n",
    "            for idx_1, idx_2 in zip(idx_list[:-1], idx_list[1:]):\n",
    "                self.assertGreaterEqual(self.cosine_scores[idx_query][idx_1], self.cosine_scores[idx_query][idx_2])\n",
    "\n",
    "    def test_retrieval_label(self):\n",
    "        # check if label of a query case and a retrieved one do share at least one label\n",
    "        for (idx_query, idx_list) in self.idx_retrieved_cases:\n",
    "            for idx, label in zip(idx_list, self.retrieval_scores[idx_query]):\n",
    "                if label: # only assert if there is a positive label in the first place\n",
    "                    self.assertGreater(len(df_all.iloc[idx][\"celex\"] & df_all.iloc[idx_query][\"celex\"]), 0)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facilex_caselaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
